{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Notebook\n",
    "\n",
    "This notebook is used to explore the data and understand through some graphs of categories and numerical characteristics and at the end to verify the importance of the characteristics with SHAP. SHAP values are the most mathematically consistent way to get resource importances and work particularly well with tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_curve, roc_auc_score, plot_roc_curve\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "# Config\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "df_train = pd.read_csv('../data/raw/train.csv')\n",
    "df_test = pd.read_csv('../data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quantity of rows and columns\n",
    "print('Quantidade de Linhas: ', df_train.shape[0])\n",
    "print('Quantidade de Colunas: ', df_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see that the train dataset is small, containing only 5634 rows, 21 variables and no *missing* value, and of these 21 variables only 3 are numerical variables.\n",
    "\n",
    "Let's start looking at an attribute plus exploratory background analysis now for these variable elements, and lastly to study the elemental attributes. We start with the bar chart of the target Churn variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target variable\n",
    "#plt.figure(figsize=(4,3))\n",
    "ax = sns.barplot(x=df_train['Churn'].value_counts().index, \n",
    "                 y=df_train['Churn'].value_counts().values,\n",
    "                 data=df_train)\n",
    "\n",
    "total = float(len(df_train))\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2., height, '{:1.2f}'.format(height/total), \n",
    "            fontsize=12, color='black', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Churn Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that most of the company's customers did not cancel the contract, about 73%. It is a good result for the company, however, for our model it is something to be corrected, because the variable is unbalanced. We will correct this later, now we will continue the exploratory analysis and cleaning of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Churn'] = df_train['Churn'].map({'Yes': 1,'No': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Category variables (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting senior citizen to object\n",
    "df_train['SeniorCitizen'] = df_train['SeniorCitizen'].astype('object')\n",
    "\n",
    "# casting total charges to float\n",
    "df_train['TotalCharges'] = df_train['TotalCharges'].apply(lambda x: str(x).replace(',', '.'))\n",
    "df_train['TotalCharges'] = pd.to_numeric(df_train['TotalCharges'], errors='coerce')\n",
    "df_train['TotalCharges'] = df_train['TotalCharges'].astype('float64')\n",
    "df_train['TotalCharges'] = df_train[\"TotalCharges\"].replace(\" \",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID\n",
    "df_train.drop('customerID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only categorical variables\n",
    "df_train_cat = df_train.select_dtypes(include=['object'])\n",
    "\n",
    "# Select only numerical variables\n",
    "df_train_quant = df_train.select_dtypes(exclude=['object'])\n",
    "df_train_quant.drop('Churn', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of categorical features have a two or three unique values, so we can use a bar chart to see the distribution of the values. We will use a function to plot the bar chart of each categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "for idx, col in enumerate(df_train_cat):\n",
    "    ax = plt.subplot(4,4,idx+1)\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    sns.countplot(x=col, data=df_train_cat) \n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width()/2., height, '{:1.2f}'.format(height/total), \n",
    "                fontsize=12, color='black', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group all categorical variables by churn then plot a horizontal stacked bar chart of the proportions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(30, 20))\n",
    "for i, col in enumerate(df_train_cat.columns):\n",
    "    df_train_cat.groupby([col, df_train['Churn']]).size().unstack().plot(kind='barh', stacked=True, ax=axes[i//4, i%4])\n",
    "    axes[i//4, i%4].set_title(col)\n",
    "    axes[i//4, i%4].set_xlabel('')\n",
    "    axes[i//4, i%4].set_ylabel('')\n",
    "    axes[i//4, i%4].set_xticklabels(axes[i//4, i%4].get_xticklabels())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two graphs above we can see that most of the categorical characteristics are unbalanced, this is clearer in the stacked bar plot and we need to understand how these variables are really important to predict the target variable.\n",
    "\n",
    "We can see that in some categorical attributes they have values like \"No phone service\" and \"No internet service\" this makes it difficult for both data visualizations and machine learning algorithms.\n",
    "\n",
    "Instead of keeping these variables with these values, let's replace them with \"No\" so we can have the data more organized and also when the dummy variables are created in the final dataset it will have a smaller dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Numerical variables (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_quant.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8, 4))\n",
    "for idx, col in enumerate(df_train[df_train_quant.columns]):\n",
    "    ax = plt.subplot(1,3,idx+1)\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    sns.distplot(df_train.loc[df_train.Churn == 0][col], hist=False, axlabel= False, \n",
    "    kde_kws={'linestyle':'-', \n",
    "             'label':\"Risk\"})\n",
    "    sns.distplot(df_train.loc[df_train.Churn == 1][col], hist=False, axlabel= False, \n",
    "    kde_kws={'linestyle':'--', \n",
    "             'label':\"No Risk\"})\n",
    "    ax.set_title(col)\n",
    "\n",
    "# Mostra o gr√°fico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the histogram below we can see that the numerical variables are not normally distributed, so we will need to normalize them before training some linear machine learning models. We can also see that distribution for clients who have churned is different from those who have not churned for the variables Tenue and MonthlyCharges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, box in zip(axes, df_train[df_train_quant.columns]):\n",
    "    sns.boxplot(x='Churn',\n",
    "                y=box, \n",
    "                ax=ax,\n",
    "                data=df_train\n",
    "    )\n",
    "\n",
    "# Mostra o gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Itera sobre as vari√°veis quantitativas para gerar o violin plot\n",
    "for ax, violin in zip(axes, df_train[df_train_quant.columns]):\n",
    "    sns.violinplot(x='Churn',\n",
    "                   y=violin, \n",
    "                   ax=ax,\n",
    "                   split=True,\n",
    "                   data=df_train)\n",
    "\n",
    "# Mostra o gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With box and violin plots we can see the distribution of the numerical variables for each class of the target variable. We can see these three variables dont have much outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_train, \n",
    "             vars=df_train[df_train_quant.columns],\n",
    "             hue='Churn', \n",
    "             diag_kind='hist', \n",
    "             corner=True, \n",
    "             plot_kws=dict(alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pairplot above we can see that the numerical variables are not normally distributed and the variable TotalCharges have a strong possitive relationship with the Tenure and MonthlyCharges variables. For check this relationship we can use a heatmap below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_train[df_train_quant.columns].corr(method='pearson'), \n",
    "            annot=True, \n",
    "            linewidths=0.3, \n",
    "            square=True)\n",
    "\n",
    "# Mostra o gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a pipeline to train and test the model. We will use the pipeline to train and test the model with the original dataset and with the dataset. We will use this to check feature importance with SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/raw/train.csv')\n",
    "X_test = pd.read_csv('../data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Churn'] = X_train['Churn'].map({'Yes': 1,'No': 0})\n",
    "X_test['Churn'] = X_test['Churn'].map({'Yes': 1,'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train['Churn']\n",
    "X_train = X_train.drop('Churn', axis=1)\n",
    "\n",
    "y_test = X_test['Churn']\n",
    "X_test = X_test.drop('Churn', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_commas(X):\n",
    "    X['TotalCharges'] = X['TotalCharges'].apply(lambda x: str(x).replace(',', '.'))\t\n",
    "    return np.array(X['TotalCharges']).reshape(-1, 1)\n",
    "\n",
    "def casting_to_float(X):\n",
    "    X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "    X['TotalCharges'] = X['TotalCharges'].astype('float64')\n",
    "    return np.array(X['TotalCharges']).reshape(-1, 1)\n",
    "\n",
    "def replace_spaces(X):\n",
    "    X['TotalCharges'] = X['TotalCharges'].apply(lambda x: str(x).replace(' ', 'np.nan'))\t\n",
    "    return np.array(X['TotalCharges']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['tenure', 'MonthlyCharges']\n",
    "\n",
    "cat_vars = [\n",
    "    var for var in X_train.columns if X_train[var].dtype == \"object\" \n",
    "    and var in ['SeniorCitizen'] \n",
    "    and var not in ['Churn', 'customerID']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple pipeline for numerical features and categorical features with a xgboost classifier\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', SimpleImputer(strategy='median'), num_vars),\n",
    "        ('cat', TargetEncoder(cols=cat_vars), cat_vars)\n",
    "    ], remainder='drop')),\n",
    "    ('classifier', xgb.XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.churn_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33756bd410fd1575cb7e1e8676668def99b13921af9d80f486eabc1b4c62c886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
