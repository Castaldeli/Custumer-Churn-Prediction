# Custumer Churn Prediction
==============================

Custumer churn prediction is a problem that many companies face. The goal of this project is to predict if a customer will churn or not.


# 1 Pre-requisites

**Ubuntu/Debian/Mint**

The GNU/Linux Make utility is required to run the Makefile. To install it, run the following command:

```bash
sudo apt-get install build-essential
```

Also, the Python 3 interpreter is required to run the project. To install it, run the following command:

```bash
sudo apt-get install python3
```

# 1.1 Environment Setup

To set up the environment, run the following command:

```bash
python3 -m venv ~/path/to/venv
```

To activate the environment, run the following command:

```bash
source ~/path/to/venv/bin/activate
```

# 1.2 Install Dependencies

To install the dependencies, run the following command:

```bash
make requirements
```

# 2 Configure kaggle API Credentials

The Kaggle's beta API key allows you to interact with Competitions and Datasets to download data, make submissions, and more via the command line.

1. Download Kaggle's beta API key

    * Go to your profile, click on your picture on the top left, and next click on "Account";
    * Scroll down and click in "Create New API Token", it will download your Kaggle's key (you can expire the token anytime clicking in "Expire API Token");

2. Configure API Credentials

    * Now, place this file in the location ~/.kaggle/kaggle.json (on Windows in the location C:\Users\<Windows-username>\.kaggle\kaggle.json you can check the exact location, sans drive, with echo %HOMEPATH%);
    * You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to $KAGGLE_CONFIG_DIR/kaggle.json (on Windows it will be %KAGGLE_CONFIG_DIR%\kaggle.json)
    * For your security, ensure that other users of your computer do not have read access to your credentials. On Unix-based systems you can do this with the following command:

```bash
chmod 600 ~/.kaggle/kaggle.json
```

You can also choose to export your Kaggle username and token to the environment:
    
```bash
cd nano ~/.bashrc
```

The create the following variables:

```bash
export KAGGLE_USERNAME=yourkaggleusername
export KAGGLE_KEY=xxxxxxxxxxxxxxxxx
```

Run the command:

```bash
source ~/.bashrc
```

check if the variables are set:

```bash
echo $KAGGLE_USERNAME
echo $KAGGLE_KEY
```

# 3 Run the Project

To run the project, run the following command:

```bash
dvc repro
```

# 4 Project Organization
------------

    ├── LICENSE                     <- Repository license
    ├── Makefile                    <- Makefile with commands like `make data` or `make train`
    ├── README.md                   <- The top-level README for developers using this project.
    ├── backend                     <- Backend folder with the API and the data expecting for the api
    │   ├── api.py                  <- API with the endpoints to predict and train the model
    │   └── churn_data.py           <- Data to be used by the API
    ├── configs.ini                 <- Config file with the parameters to be used by the API
    ├── data
    │   ├── interim                 <- Intermediate data that has been transformed.
    │   ├── processed               <- The final, canonical data sets for modeling.
    │   └── raw                     <- The original, immutable data dump.
    ├── docs                        <- A default Sphinx project; see sphinx-doc.org for details
    ├── dvc.lock                    <- DVC lock file used to record the state of the pipeline and its outputs
    ├── dvc.yaml                    <- DVC pipeline file. Defines the machine learning pipeline and the stages
    ├── frontend                    <- Frontend folder with the web app
    │   └── app.py                  <- Web app with the interface to predict and train the model
    ├── models                      <- Trained and serialized models
    ├── notebooks                   <- Jupyter notebooks used to experiment and develop the project
    ├── params.yaml                 <- Parameters used by the DVC pipeline
    ├── references                  <- Data dictionaries, manuals, and all other explanatory materials.
    ├── reports                     <- Generated analysis as HTML, PDF, LaTeX, etc.
    ├── requirements-dev.in         <- The file where you should put the development dependencies like Black, Pytest, etc.
    ├── requirements-dev.txt        <- ❗This file will be automatically generated by the pip-compile, DON'T EDIT IT❗
    ├── requirements.in             <- The file where you should put the modules and scripts dependencies.
    ├── requirements.txt            <- ❗This file will be automatically generated by the pip-compile, DON'T EDIT IT❗
    ├── setup.py                    <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                         <- Source code for use in this project.
    │   ├── data                    <- Scripts to download or generate data
    │   │   ├── make_dataset.py     <- Script to download the data from the internet and save it in the raw folder
    │   │   └── split_data.py       <- Module used to split the data into train and test in make_dataset.py
    │   ├── features                <- Scripts to turn raw data into features for modeling
    │   │   ├── build_features.py   <- Script to generate the features from the raw data
    │   │   └── casting_vars.py     <- Module used to cast the variables in build_features.py
    │   ├── models                  <- Scripts to train models and then use trained models to make
    │   │   ├── evaluate_model.py   <- Script to evaluate the model
    │   │   ├── plot_metric.py      <- Module used to plot the metrics in evaluate_model.py
    │   │   ├── score_model.py      <- Module used to score the model in evaluate_model.py
    │   │   └── train_model.py      <- Script to train the model
    │   └── visualization           <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py        <- Script to create the plots
    ├── test_environment.py         <- Script to test if the environment is correctly set up
    └── tox.ini                     <- tox file with settings for running tox; see tox.testrun.org
------------

# 5 Test model with the API

To test the model with the API, run the following command:

```bash
cd backend

uvicorn api:app --reload
```

After that you can open in another terminal the UI to test the API with the following command:

```bash
cd frontend

streamlit run app.py
```


<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
